{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data\\penguins.csv\n",
      "../data\\nlp-getting-started\\sample_submission.csv\n",
      "../data\\nlp-getting-started\\test.csv\n",
      "../data\\nlp-getting-started\\train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# tensorflow hub\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# tensor flow module\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tensorflow_hub\\__init__.py:111\u001b[0m\n\u001b[0;32m    103\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    104\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to import tf_keras. Please note that tf_keras is not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m installed by default when you install tensorflow_hub. This is so\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m that users can decide which tf_keras package to use. To use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tensorflow_hub, please install a current version of tf_keras.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m       )\n\u001b[0;32m    109\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m \u001b[43m_ensure_keras_2_importable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Symbols exposed via tensorflow_hub.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLayer\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tensorflow_hub\\__init__.py:100\u001b[0m, in \u001b[0;36m_ensure_keras_2_importable\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_fn \u001b[38;5;129;01mand\u001b[39;00m version_fn()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m    101\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# Print more informative error message, then reraise.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to import tf_keras. Please note that tf_keras is not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m installed by default when you install tensorflow_hub. This is so\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m that users can decide which tf_keras package to use. To use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tensorflow_hub, please install a current version of tf_keras.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m     )\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the TF-Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\applications\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\applications\\convnext.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imagenet_utils\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sequential\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\sequential.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers \u001b[38;5;28;01mas\u001b[39;00m layer_module\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_layer\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_spec\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_utils\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v1\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pickle_utils\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_api\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\saving\\saving_api.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m legacy_sm_saving_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\save.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m saved_model_load\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_context\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m saved_model_save\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_option_scope\n",
      "File \u001b[1;32md:\\Code\\2024-2025 sem 1\\Machine-Learning-20242025Sem1\\.venv\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\load_context.py:68\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether under a load context.\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_context\u001b[38;5;241m.\u001b[39min_load_context()\n\u001b[1;32m---> 68\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_load_context_function\u001b[49m(in_load_context)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'"
     ]
    }
   ],
   "source": [
    "# tensorflow hub\n",
    "import tensorflow_hub as hub\n",
    "# tensor flow module\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# matplotlib\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# word vectorizor\n",
    "# first converts the text into a matrix of word counts\n",
    "# then transforms these counts by normalizing them based on the term frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# used to create word encoders\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "In this competition we have a keyword(sometimes), location(sometimes), and text to represent a Tweet. Our goal is to predict whether the tweet is describing a real disaster (1) or not (0).\n",
    "\n",
    "In this kernel I am going to focus on the text and keyword features to predict a target. First, I must convert the text into vectors to use in my model. I will use a multinominal Naive Bayes classifier to build my model given the text features. Then I will use Naive Bayes to get predictions based on keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow code is inspired by github repo:\n",
    "https://github.com/nicolov/naive_bayes_tensorflow/blob/master/tf_iris.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in data to pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory\n",
    "I just want to get a general idea of the data before we predict the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count how many of each target is found in the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(\"target\")[\"id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is amazing! **Our training set has no null targets and our targets are fairly balanced as well!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique locations are duplicated in the tweets of the tweets that contain locations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2158\n",
      "1602\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df.loc[test_df[\"location\"].notnull(),\"location\"]))\n",
    "print(len(test_df.loc[test_df[\"location\"].notnull(),\"location\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like not even half match a previous tweet. We may eventually have to categorize locations using a different technique than string matching. For example, it may be useful to categorize cities verse suburban areas rather than just the city names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the key words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3237\n",
      "221\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df.loc[test_df[\"keyword\"].notnull(),\"keyword\"]))\n",
    "print(len(test_df.loc[test_df[\"keyword\"].notnull(),\"keyword\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These may actually be useful in a naive bayes model since each key word will have a conditional probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Text: Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can convert each of the texts into vectors using a TensorFlow universal-sentence-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/3\")\n",
    "X_train_embeddings = embed(train_df[\"text\"].values)\n",
    "X_test_embeddings = embed(test_df[\"text\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow Naiver Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create numpy matricies for the features in the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_matrix = X_train_embeddings['outputs'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_matrix = X_test_embeddings['outputs'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is my y variable for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.constant(train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a python object that can fit and predict a naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNaiveBayesClassifier:\n",
    "    dist = None\n",
    "    \n",
    "    # X is the matrix containing the vectors for each sentence\n",
    "    # y is the list target values in the same order as the X matrix\n",
    "    def fit(self, X, y):\n",
    "        unique_y = np.unique(y) # unique target values: 0,1\n",
    "        print(unique_y)\n",
    "        # `points_by_class` is a numpy array the size of \n",
    "        # the number of unique targets.\n",
    "        # in each item of the list is another list that contains the vector\n",
    "        # of each sentence from the same target value\n",
    "        points_by_class = np.asarray([np.asarray(\n",
    "            [np.asarray(\n",
    "                X.iloc[x,:]) for x in range(0,len(y)) if y[x] == c]) for c in unique_y])\n",
    "        mean_list=[]\n",
    "        var_list=[]\n",
    "        for i in range(0, len(points_by_class)):\n",
    "            mean_var, var_var = tf.nn.moments(tf.constant(points_by_class[i]), axes=[0])\n",
    "            mean_list.append(mean_var)\n",
    "            var_list.append(var_var)\n",
    "        mean=tf.stack(mean_list, 0)\n",
    "        var=tf.stack(var_list, 0)\n",
    "        # Create a 3x2 univariate normal distribution with the \n",
    "        # known mean and variance\n",
    "        self.dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        assert self.dist is not None\n",
    "        nb_classes, nb_features = map(int, self.dist.scale.shape)\n",
    "\n",
    "        # uniform priors\n",
    "        priors = np.log(np.array([1. / nb_classes] * nb_classes))\n",
    "        \n",
    "        # Conditional probabilities log P(x|c)\n",
    "        # (nb_samples, nb_classes, nb_features)\n",
    "        all_log_probs = self.dist.log_prob(\n",
    "            tf.reshape(\n",
    "                tf.tile(X, [1, nb_classes]), [-1, nb_classes, nb_features]))\n",
    "        # (nb_samples, nb_classes)\n",
    "        cond_probs = tf.reduce_sum(all_log_probs, axis=2)\n",
    "        \n",
    "        # posterior log probability, log P(c) + log P(x|c)\n",
    "        joint_likelihood = tf.add(priors, cond_probs)\n",
    "\n",
    "        # normalize to get (log)-probabilities\n",
    "        norm_factor = tf.reduce_logsumexp(\n",
    "            joint_likelihood, axis=1, keepdims=True)\n",
    "        log_prob = joint_likelihood - norm_factor\n",
    "        # exp to get the actual probabilities\n",
    "        return tf.exp(log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize our naive bayes model and fit it using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "tf_nb = TFNaiveBayesClassifier()\n",
    "tf_nb.fit(pd.DataFrame(X_train_matrix),\n",
    "          y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict probability of each target values in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf_nb.predict(X_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe containing the probability of each target given the text in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.993159e-01</td>\n",
       "      <td>0.200685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.958143e-09</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.392048e-04</td>\n",
       "      <td>0.999390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.166333e-13</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.064461e-18</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0  7.993159e-01  0.200685\n",
       "1  1.958143e-09  1.000000\n",
       "2  6.392048e-04  0.999390\n",
       "3  6.166333e-13  1.000000\n",
       "4  7.064461e-18  1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predProbGivenText_df = pd.DataFrame(y_pred.numpy())\n",
    "predProbGivenText_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our unique key words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "['ablaze' 'accident' 'aftershock' 'airplane%20accident' 'ambulance'\n",
      " 'annihilated' 'annihilation' 'apocalypse' 'armageddon' 'army' 'arson'\n",
      " 'arsonist' 'attack' 'attacked' 'avalanche' 'battle' 'bioterror'\n",
      " 'bioterrorism' 'blaze' 'blazing' 'bleeding' 'blew%20up' 'blight'\n",
      " 'blizzard' 'blood' 'bloody' 'blown%20up' 'body%20bag' 'body%20bagging'\n",
      " 'body%20bags' 'bomb' 'bombed' 'bombing' 'bridge%20collapse'\n",
      " 'buildings%20burning' 'buildings%20on%20fire' 'burned' 'burning'\n",
      " 'burning%20buildings' 'bush%20fires' 'casualties' 'casualty'\n",
      " 'catastrophe' 'catastrophic' 'chemical%20emergency' 'cliff%20fall'\n",
      " 'collapse' 'collapsed' 'collide' 'collided' 'collision' 'crash' 'crashed'\n",
      " 'crush' 'crushed' 'curfew' 'cyclone' 'damage' 'danger' 'dead' 'death'\n",
      " 'deaths' 'debris' 'deluge' 'deluged' 'demolish' 'demolished' 'demolition'\n",
      " 'derail' 'derailed' 'derailment' 'desolate' 'desolation' 'destroy'\n",
      " 'destroyed' 'destruction' 'detonate' 'detonation' 'devastated'\n",
      " 'devastation' 'disaster' 'displaced' 'drought' 'drown' 'drowned'\n",
      " 'drowning' 'dust%20storm' 'earthquake' 'electrocute' 'electrocuted'\n",
      " 'emergency' 'emergency%20plan' 'emergency%20services' 'engulfed'\n",
      " 'epicentre' 'evacuate' 'evacuated' 'evacuation' 'explode' 'exploded'\n",
      " 'explosion' 'eyewitness' 'famine' 'fatal' 'fatalities' 'fatality' 'fear'\n",
      " 'fire' 'fire%20truck' 'first%20responders' 'flames' 'flattened' 'flood'\n",
      " 'flooding' 'floods' 'forest%20fire' 'forest%20fires' 'hail' 'hailstorm'\n",
      " 'harm' 'hazard' 'hazardous' 'heat%20wave' 'hellfire' 'hijack' 'hijacker'\n",
      " 'hijacking' 'hostage' 'hostages' 'hurricane' 'injured' 'injuries'\n",
      " 'injury' 'inundated' 'inundation' 'landslide' 'lava' 'lightning'\n",
      " 'loud%20bang' 'mass%20murder' 'mass%20murderer' 'massacre' 'mayhem'\n",
      " 'meltdown' 'military' 'mudslide' 'natural%20disaster'\n",
      " 'nuclear%20disaster' 'nuclear%20reactor' 'obliterate' 'obliterated'\n",
      " 'obliteration' 'oil%20spill' 'outbreak' 'pandemonium' 'panic' 'panicking'\n",
      " 'police' 'quarantine' 'quarantined' 'radiation%20emergency' 'rainstorm'\n",
      " 'razed' 'refugees' 'rescue' 'rescued' 'rescuers' 'riot' 'rioting'\n",
      " 'rubble' 'ruin' 'sandstorm' 'screamed' 'screaming' 'screams' 'seismic'\n",
      " 'sinkhole' 'sinking' 'siren' 'sirens' 'smoke' 'snowstorm' 'storm'\n",
      " 'stretcher' 'structural%20failure' 'suicide%20bomb' 'suicide%20bomber'\n",
      " 'suicide%20bombing' 'sunk' 'survive' 'survived' 'survivors' 'terrorism'\n",
      " 'terrorist' 'threat' 'thunder' 'thunderstorm' 'tornado' 'tragedy'\n",
      " 'trapped' 'trauma' 'traumatised' 'trouble' 'tsunami' 'twister' 'typhoon'\n",
      " 'upheaval' 'violent%20storm' 'volcano' 'war%20zone' 'weapon' 'weapons'\n",
      " 'whirlwind' 'wild%20fires' 'wildfire' 'windstorm' 'wounded' 'wounds'\n",
      " 'wreck' 'wreckage' 'wrecked']\n"
     ]
    }
   ],
   "source": [
    "uniq_keywords = train_df[\"keyword\"].unique()[1:]\n",
    "print(len(uniq_keywords))\n",
    "print(uniq_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that there are some keywords that are very similar to each other so I decided to manually make them the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_keywords(df_og):\n",
    "    df = df_og.copy()\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"ablaze\",\"blaze\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"blazing\",\"blaze\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"annihilated\",\"annihilation\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"attacked\",\"attack\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bioterror\",\"bioterrorism\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"blown%20up\",\"blew%20up\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bloody\",\"blood\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bleeding\",\"blood\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bags\",\"body%20bag\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bagging\",\"body%20bag\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bombed\",\"bomb\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bombing\",\"bomb\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"burning%20buildings\",\"buildings%20burning\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"buildings%20on%20fire\",\"buildings%20burning\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"burned\",\"burning\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"casualties\",\"casualty\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"catastrophe\",\"catastrophic\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"collapse\",\"collapsed\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"collide\",\"collision\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"collided\",\"collision\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"crash\",\"crashed\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"crush\",\"crushed\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"dead\",\"death\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"deaths\",\"death\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"deluge\",\"deluged\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"demolished\",\"demolish\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"demolition\",\"demolish\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"derailment\",\"derail\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"derailed\",\"derail\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"desolation\",\"desolate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"destroyed\",\"destroy\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"destruction\",\"destroy\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"detonate\",\"detonation\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"devastated\",\"devastation\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"drowned\",\"drown\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"drowning\",\"drown\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"electrocute\",\"electrocuted\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuated\",\"evacuate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuation\",\"evacuate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"explode\",\"explosion\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"exploded\",\"explosion\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"fatality\",\"fatalities\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"floods\",\"flood\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"flooding\",\"flood\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"bush%20fires\",\"forest%20fire\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"forest%20fires\",\"forest%20fire\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hailstorm\",\"hail\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hazardous\",\"hazard\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacking\",\"hijack\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacker\",\"hijack\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"hostages\",\"hostage\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"injured\",\"injury\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"injures\",\"injury\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"inundated\",\"inundation\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"mass%20murderer\",\"mass%20murder\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"obliterated\",\"obliterate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"obliteration\",\"obliterate\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"panicking\",\"panic\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"quarantined\",\"quarantine\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"rescuers\",\"rescue\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"rescued\",\"rescue\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"rioting\",\"riot\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"dust%20storm\",\"sandstorm\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"screamed\",\"screams\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"screaming\",\"screams\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"sirens\",\"siren\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bomb\",\"suicide%20bomber\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bombing\",\"suicide%20bomber\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"survived\",\"survive\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"survivors\",\"survive\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"terrorism\",\"terrorist\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"thunderstorm\",\"thunder\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"traumatised\",\"trauma\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"twister\",\"tornado\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"typhoon\",\"hurricane\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"weapons\",\"weapon\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"wild%20fires\",\"wildfire\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"wounded\",\"wounds\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"wrecked\",\"wreckage\")\n",
    "    df[\"keyword\"] = df[\"keyword\"].replace(\"wreck\",\"wreckage\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = replace_keywords(train_df)\n",
    "test_df = replace_keywords(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the updated set of keywords let's get the probability for each target given a specific key word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>keywordPred0</th>\n",
       "      <th>keywordPred1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blaze</td>\n",
       "      <td>0.824074</td>\n",
       "      <td>0.175926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accident</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aftershock</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ambulance</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               keyword  keywordPred0  keywordPred1\n",
       "0                blaze      0.824074      0.175926\n",
       "0             accident      0.314286      0.685714\n",
       "0           aftershock      1.000000      0.000000\n",
       "0  airplane%20accident      0.142857      0.857143\n",
       "0            ambulance      0.473684      0.526316"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_keywords = train_df[\"keyword\"].unique()[1:]\n",
    "kword_resArr = []\n",
    "print(len(uniq_keywords))\n",
    "for kword in uniq_keywords:\n",
    "    kword_df = train_df.loc[train_df[\"keyword\"] == kword,: ]\n",
    "    total_kword = float(len(kword_df))\n",
    "    target0_n = float(len(kword_df.loc[kword_df[\"target\"]==0,:]))\n",
    "    target1_n = float(len(kword_df.loc[kword_df[\"target\"]==1,:]))\n",
    "    kword_prob_df = pd.DataFrame({'keyword':[kword],\n",
    "                                 \"keywordPred0\": [target0_n/total_kword],\n",
    "                                 \"keywordPred1\": [target1_n/total_kword]})\n",
    "    kword_resArr.append(kword_prob_df)\n",
    "predProbGivenKeyWord_df= pd.concat(kword_resArr)\n",
    "predProbGivenKeyWord_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get probabilities given the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>textprob0</th>\n",
       "      <th>textprob1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>7.993159e-01</td>\n",
       "      <td>0.200685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1.958143e-09</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>6.392048e-04</td>\n",
       "      <td>0.999390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>6.166333e-13</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>7.064461e-18</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "      textprob0  textprob1  \n",
       "0  7.993159e-01   0.200685  \n",
       "1  1.958143e-09   1.000000  \n",
       "2  6.392048e-04   0.999390  \n",
       "3  6.166333e-13   1.000000  \n",
       "4  7.064461e-18   1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"textprob0\"]=predProbGivenText_df.loc[:,0].copy()\n",
    "test_df[\"textprob1\"]=predProbGivenText_df.loc[:,1].copy()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the probabilities given the key words. Note that if there is no key word than the probability is 50% for both targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.merge(predProbGivenKeyWord_df, how='left', on=\"keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"keywordPred0\"]=test_df[\"keywordPred0\"].fillna(0.5)\n",
    "test_df[\"keywordPred1\"]=test_df[\"keywordPred1\"].fillna(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the probability given the text and the keyword. Then let's choose our prediction based on which target has the higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"pred0\"]=test_df[\"textprob0\"]*test_df[\"keywordPred0\"]\n",
    "test_df[\"pred1\"]=test_df[\"textprob1\"]*test_df[\"keywordPred1\"]\n",
    "test_df[\"target\"]=test_df[\"pred1\"]>test_df[\"pred0\"]\n",
    "test_df[\"target\"] = test_df[\"target\"].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = test_df.loc[:,[\"id\",\"target\"]]\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the code above I only look at the tweets themselves and the keywords, but many of the tweets do not have keywords. We could maybe create a model to infer keywords from tweets that do not have keywords. We also did not include location in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
